#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import logging.config
import pathlib
from pprint import pformat
from typing import List, Dict

import common.Graph.graph_datastructure as graph_datastructure
import common.ImportExport.json_import_export as json_import_export
from carlhauser_client.API.extended_api import Extended_API
from carlhauser_client.EvaluationTools.StorageGraphExtractor.cluster_matching_quality_evaluator import ClusterMatchingQualityEvaluator
from common.ChartMaker.confusion_matrix_generator import ConfusionMatrixGenerator
from common.Graph.cluster import Cluster
from common.PerformanceDatastructs.clustermatch_datastruct import ClusterMatch
from common.environment_variable import get_homedir
from common.environment_variable import load_client_logging_conf_file

load_client_logging_conf_file()


# ==================== ------ LAUNCHER ------- ====================
class InternalClusteringQualityEvaluator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.API: Extended_API = Extended_API.get_api()

    def get_storage_graph(self, image_folder: pathlib.Path, visjs_json_path: pathlib.Path, output_path: pathlib.Path) -> Dict:
        '''
        Extract a storage graph from a folder of pictures, sent to DB and a dump request to the DB.
        Store all pictures in the server and dump the database as is.
        :param image_folder: The folder of picture to send and request, to build the storage graph from
        :param visjs_json_path:
        :param output_path: The output path where the graph and other data will be stored
        :return:
        '''

        # ========= MANUAL EVALUATION =========

        # Load ground truth file
        gt_graph = graph_datastructure.load_visjs_to_graph(visjs_json_path)

        # ========= AUTO EVALUATION =========
        # Send pictures to DB and get id mapping
        mapping_old_filename_to_new_id, nb_pictures = self.API.add_many_picture_and_wait_for_each(image_folder)

        # Get a DB dump
        db_dump = self.API.get_db_dump_as_graph()

        # ========= COMPARISON =========
        # Apply name mapping to dict (find back original names)
        gt_graph.replace_id_from_mapping(mapping_old_filename_to_new_id)

        # Extract the lists of clusters
        candidate = db_dump.get_clusters()
        original = gt_graph.get_clusters()

        # Match clusters
        # 1. Manually ? (Go back to visjs + rename manually to have the same matching names)
        # 2. Automatically ? (Match on number of common elements ~, see function)
        # 2 was chosen.

        # Groups clusters per pair, by matching original clusters with Candidate clusters
        matching = self.match_clusters(original, candidate)

        # Compute from each pair of clusters their Quality score
        matching_with_perf = ClusterMatchingQualityEvaluator.evaluate_performance(matching, nb_pictures)

        # Store performance in a file
        perfs = ClusterMatchingQualityEvaluator.export_as_json(matching_with_perf)

        save_path_perf = output_path / "perf.json"
        json_import_export.save_json(perfs, save_path_perf)
        self.logger.debug(f"Json saved in : {save_path_perf}")

        # ========= RESULT VISUALIZATON =========

        # Convert matching with performance to confusion matrix
        matrix_creator = ConfusionMatrixGenerator()
        matrix_creator.create_and_export_confusion_matrix(original, candidate, output_path / "matrix.pdf")

        # Convert dumped graph to visjs graphe
        # ==> red if linked made by algo, but non existant + Gray, true link that should have been created (
        # ==> Green if linked made by algo and existant
        output_graph = graph_datastructure.merge_graphs(gt_graph, db_dump, matching)

        save_path_json = output_path / "merged_graph.json"
        json_import_export.save_json(output_graph, save_path_json)
        self.logger.debug(f"DB Dump json saved in : {save_path_json}")

        # ==============================

        return perfs

    def match_clusters(self, original: List[Cluster], candidate: List[Cluster]) -> List[ClusterMatch]:
        '''
        Automatically match clusters together. From two list of Clusters, provides a List of cluster matches (pair of clusters)
        TODO : Handle a pairing with unicity of each cluster in the paired list. Here a cluster can be matched with one other that was already matched with another one.
        :param original: List of clusters generated by the library/server/ ...
        :param candidate: List of clusters extracted from ground truth
        :return: List of pair of clusters, matched together
        '''

        self.logger.debug(f"Inputs : \n original = {pformat(original)} \n original = {pformat(candidate)}")

        # Sort arrays (bigger to smaller)
        original.sort(key=lambda x: len(x.members), reverse=True)
        candidate.sort(key=lambda x: len(x.members), reverse=True)

        self.logger.debug(f"Sorted : \n original = {pformat(original)} \n original = {pformat(candidate)}")

        matching = []

        # For each ground truth cluster
        for curr_original_cluster in original:
            max_intersect = 0
            index_best_intersect = -1

            # For each server-extracted cluster
            for i, curr_candidat_cluster in enumerate(candidate):

                # If the size of the current cluster is below the best seen intersect =
                # The current cluster and the next ones can't be better
                if len(curr_candidat_cluster.members) < max_intersect:

                    # Remove current cluster from matching
                    # TODO : if index_best_intersect != -1 :
                    #  candidate.remove(index_best_intersect)
                    # else :
                    # "No candidate found for ... "

                    # As the cluster are sorted by size, if we haven't found any "good" cluster, we can just stop !
                    if index_best_intersect == -1:
                        self.logger.debug(f"No candidate found to match cluster = {curr_original_cluster}")
                    break  # So we stop
                else:
                    # Compute intersection
                    tmp = len(curr_original_cluster.members.intersection(curr_candidat_cluster.members))

                    # Store if better =  If the current intersection of both clusters is greater than the one already seen
                    if tmp > max_intersect:
                        max_intersect = tmp
                        index_best_intersect = i

            # Store best matching
            matching.append(ClusterMatch(curr_original_cluster, candidate[index_best_intersect]))

        self.logger.debug(f"matching : \n {pformat(matching)}")

        # TODO : Problem with clusters that are not matched.
        # What to do with them ? Let them ? Match them by force ? ...
        return matching


'''
def main():
    parser = argparse.ArgumentParser(description='Perform an evaluation on a dataset : Send all pictures, ')
    parser.add_argument('-p', '--path', dest='path', action='store', type=lambda p: pathlib.Path(p).absolute(), default=1, help='all path')
    parser.add_argument('--version', action='version', version='humanizer %s' % ("1.0.0"))

    args = parser.parse_args()
    humanizer = Humanizer()
    humanizer.rename_all_files(args.path)

'''


def test():
    evaluator = InternalClusteringQualityEvaluator()
    image_folder = get_homedir() / "datasets" / "MINI_DATASET"
    gt = get_homedir() / "datasets" / "MINI_DATASET_VISJS.json"
    output_path = get_homedir() / "carlhauser_client"
    evaluator.get_storage_graph(image_folder, gt, output_path)


if __name__ == "__main__":
    test()
